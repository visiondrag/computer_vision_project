{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/visiondrag/computer_vision_project.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IPog56t10XX",
        "outputId": "61bd107b-b4bb-4f73-abed-194c04fb51d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'computer_vision_project'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 20 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (20/20), 4.27 MiB | 5.33 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1Jqs_4RI1e4f_Oa2bGUcrYUnlOtPpZOf8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6s6I3nZ4-Gp",
        "outputId": "7e5d0eba-9f88-4f4c-de15-a5160021be1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Jqs_4RI1e4f_Oa2bGUcrYUnlOtPpZOf8\n",
            "To: /content/shape_predictor_68_face_landmarks.dat\n",
            "100% 99.7M/99.7M [00:01<00:00, 70.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib\n",
        "import numpy as np\n",
        "import scipy.ndimage\n",
        "import os\n",
        "import PIL.Image\n",
        "import sys\n",
        "import bz2\n",
        "import argparse\n",
        "import multiprocessing\n",
        "import cv2\n",
        "import math\n",
        "from subprocess import Popen, PIPE\n",
        "from PIL import Image\n",
        "import random\n",
        "import glob\n",
        "import subprocess\n",
        "import shutil\n",
        "from skimage import io\n",
        "from imutils import face_utils"
      ],
      "metadata": {
        "id": "tWKALk4f1S8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LandmarksDetector:\n",
        "    def __init__(self, predictor_model_path='/content/shape_predictor_68_face_landmarks.dat'):\n",
        "        self.detector = dlib.get_frontal_face_detector()\n",
        "        self.shape_predictor = dlib.shape_predictor(predictor_model_path)\n",
        "\n",
        "    def get_landmarks(self, image):\n",
        "        img = dlib.load_rgb_image(image)\n",
        "        dets = self.detector(img, 1)\n",
        "\n",
        "        for detection in dets:\n",
        "            try:\n",
        "                face_landmarks = [(item.x, item.y) for item in self.shape_predictor(img, detection).parts()]\n",
        "                yield face_landmarks\n",
        "            except:\n",
        "                print(\"Exception in get_landmarks()!\")"
      ],
      "metadata": {
        "id": "uwmnVa1nn1Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_align(src_file, dst_file, face_landmarks, output_size=1024, transform_size=4096, enable_padding=True, x_scale=1, y_scale=1, em_scale=0.1, alpha=False):\n",
        "        # Align function from FFHQ dataset pre-processing step\n",
        "        # https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py\n",
        "\n",
        "        lm = np.array(face_landmarks)\n",
        "        lm_chin          = lm[0  : 17]  # left-right\n",
        "        lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
        "        lm_eyebrow_right = lm[22 : 27]  # left-right\n",
        "        lm_nose          = lm[27 : 31]  # top-down\n",
        "        lm_nostrils      = lm[31 : 36]  # top-down\n",
        "        lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
        "        lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
        "        lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
        "        lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
        "\n",
        "        # Calculate auxiliary vectors.\n",
        "        eye_left     = np.mean(lm_eye_left, axis=0)\n",
        "        eye_right    = np.mean(lm_eye_right, axis=0)\n",
        "        eye_avg      = (eye_left + eye_right) * 0.5\n",
        "        eye_to_eye   = eye_right - eye_left\n",
        "        mouth_left   = lm_mouth_outer[0]\n",
        "        mouth_right  = lm_mouth_outer[6]\n",
        "        mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
        "        eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "        # Choose oriented crop rectangle.\n",
        "        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "        x /= np.hypot(*x)\n",
        "        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "        x *= x_scale\n",
        "        y = np.flipud(x) * [-y_scale, y_scale]\n",
        "        c = eye_avg + eye_to_mouth * em_scale\n",
        "        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "        qsize = np.hypot(*x) * 2\n",
        "\n",
        "        # Load in-the-wild image.\n",
        "        if not os.path.isfile(src_file):\n",
        "            print('\\nCannot find source image. Please run \"--wilds\" before \"--align\".')\n",
        "            return\n",
        "        img = PIL.Image.open(src_file).convert('RGBA').convert('RGB')\n",
        "\n",
        "        # Shrink.\n",
        "        shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "        if shrink > 1:\n",
        "            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "            img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "            quad /= shrink\n",
        "            qsize /= shrink\n",
        "\n",
        "        # Crop.\n",
        "        border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "            img = img.crop(crop)\n",
        "            quad -= crop[0:2]\n",
        "\n",
        "        # Pad.\n",
        "        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "        if enable_padding and max(pad) > border - 4:\n",
        "            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "            h, w, _ = img.shape\n",
        "            y, x, _ = np.ogrid[:h, :w, :1]\n",
        "            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "            blur = qsize * 0.02\n",
        "            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "            img = np.uint8(np.clip(np.rint(img), 0, 255))\n",
        "            if alpha:\n",
        "                mask = 1-np.clip(3.0 * mask, 0.0, 1.0)\n",
        "                mask = np.uint8(np.clip(np.rint(mask*255), 0, 255))\n",
        "                img = np.concatenate((img, mask), axis=2)\n",
        "                img = PIL.Image.fromarray(img, 'RGBA')\n",
        "            else:\n",
        "                img = PIL.Image.fromarray(img, 'RGB')\n",
        "            quad += pad[:2]\n",
        "\n",
        "        # Transform.\n",
        "        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "        if output_size < transform_size:\n",
        "            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "        # Save aligned image.\n",
        "        img.save(dst_file, 'PNG')"
      ],
      "metadata": {
        "id": "wXFSvfM_oWw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir image\n",
        "!mkdir aligned_dir\n",
        "!cp -r /content/computer_vision_project/triangle_mesh_warping/human0.png /content/image\n",
        "!cp -r /content/computer_vision_project/triangle_mesh_warping/human1.png /content/image"
      ],
      "metadata": {
        "id": "uHWFtHtsrY_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unpack_bz2(src_path):\n",
        "    data = bz2.BZ2File(src_path).read()\n",
        "    dst_path = src_path[:-4]\n",
        "    with open(dst_path, 'wb') as fp:\n",
        "        fp.write(data)\n",
        "    return dst_path\n",
        "\n",
        "raw_dir = \"/content/image\"\n",
        "aligned_dir = \"/content/aligned_dir\"\n",
        "\n",
        "landmarks_detector = LandmarksDetector(\"/content/shape_predictor_68_face_landmarks.dat\")"
      ],
      "metadata": {
        "id": "eZwRzhCbo2z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img_name in os.listdir(raw_dir):\n",
        "  raw_img_path = os.path.join(raw_dir, img_name)\n",
        "  fn = face_img_name = '%s_%02d.png' % (os.path.splitext(img_name)[0], 1)\n",
        "  for i, face_landmarks in enumerate(landmarks_detector.get_landmarks(raw_img_path), start=1):\n",
        "    face_img_name = '%s_%02d.png' % (os.path.splitext(img_name)[0], i)\n",
        "    aligned_face_path = os.path.join(aligned_dir, face_img_name)\n",
        "    image_align(raw_img_path, aligned_face_path, face_landmarks, output_size=1024, x_scale=1, y_scale=1, em_scale=0.1, alpha=False)"
      ],
      "metadata": {
        "id": "rMBGn7D_rm7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply affine transform calculated using srcTri and dstTri to src and\n",
        "# output an image of size.\n",
        "def apply_affine_transform(src, srcTri, dstTri, size) :\n",
        "    \n",
        "    # Given a pair of triangles, find the affine transform.\n",
        "    warpMat = cv2.getAffineTransform(np.float32(srcTri), np.float32(dstTri))\n",
        "    \n",
        "    # Apply the Affine Transform just found to the src image\n",
        "    dst = cv2.warpAffine(src, warpMat, (size[0], size[1]), None, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
        "\n",
        "    return dst\n",
        "\n",
        "\n",
        "# Warps and alpha blends triangular regions from img1 and img2 to img\n",
        "def morph_triangle(img1, img2, img, t1, t2, t, alpha) :\n",
        "\n",
        "    # Find bounding rectangle for each triangle\n",
        "    r1 = cv2.boundingRect(np.float32([t1]))\n",
        "    r2 = cv2.boundingRect(np.float32([t2]))\n",
        "    r = cv2.boundingRect(np.float32([t]))\n",
        "\n",
        "    # Offset points by left top corner of the respective rectangles\n",
        "    t1Rect = []\n",
        "    t2Rect = []\n",
        "    tRect = []\n",
        "\n",
        "    for i in range(0, 3):\n",
        "        tRect.append(((t[i][0] - r[0]),(t[i][1] - r[1])))\n",
        "        t1Rect.append(((t1[i][0] - r1[0]),(t1[i][1] - r1[1])))\n",
        "        t2Rect.append(((t2[i][0] - r2[0]),(t2[i][1] - r2[1])))\n",
        "\n",
        "    # Get mask by filling triangle\n",
        "    mask = np.zeros((r[3], r[2], 3), dtype = np.float32)\n",
        "    cv2.fillConvexPoly(mask, np.int32(tRect), (1.0, 1.0, 1.0), 16, 0)\n",
        "\n",
        "    # Apply warpImage to small rectangular patches\n",
        "    img1Rect = img1[r1[1]:r1[1] + r1[3], r1[0]:r1[0] + r1[2]]\n",
        "    img2Rect = img2[r2[1]:r2[1] + r2[3], r2[0]:r2[0] + r2[2]]\n",
        "\n",
        "    size = (r[2], r[3])\n",
        "    warpImage1 = apply_affine_transform(img1Rect, t1Rect, tRect, size)\n",
        "    warpImage2 = apply_affine_transform(img2Rect, t2Rect, tRect, size)\n",
        "\n",
        "    # Alpha blend rectangular patches\n",
        "    imgRect = (1.0 - alpha) * warpImage1 + alpha * warpImage2\n",
        "\n",
        "    # Copy triangular region of the rectangular patch to the output image\n",
        "    img[r[1]:r[1]+r[3], r[0]:r[0]+r[2]] = img[r[1]:r[1]+r[3], r[0]:r[0]+r[2]] * ( 1 - mask ) + imgRect * mask\n",
        "\n",
        "\n",
        "def generate_morph_sequence(duration,frame_rate,img1,img2,points1,points2,tri_list,size,output):\n",
        "\n",
        "    num_images = int(duration*frame_rate)\n",
        "    p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-r', str(frame_rate),'-s',str(size[1])+'x'+str(size[0]), '-i', '-', '-c:v', 'libx264', '-crf', '25','-vf','scale=trunc(iw/2)*2:trunc(ih/2)*2','-pix_fmt','yuv420p', output], stdin=PIPE)\n",
        "    \n",
        "    for j in range(0, num_images):\n",
        "\n",
        "        # Convert Mat to float data type\n",
        "        img1 = np.float32(img1)\n",
        "        img2 = np.float32(img2)\n",
        "\n",
        "        # Read array of corresponding points\n",
        "        points = []\n",
        "        alpha = j/(num_images-1)\n",
        "\n",
        "        # Compute weighted average point coordinates\n",
        "        for i in range(0, len(points1)):\n",
        "            x = (1 - alpha) * points1[i][0] + alpha * points2[i][0]\n",
        "            y = (1 - alpha) * points1[i][1] + alpha * points2[i][1]\n",
        "            points.append((x,y))\n",
        "        \n",
        "        # Allocate space for final output\n",
        "        morphed_frame = np.zeros(img1.shape, dtype = img1.dtype)\n",
        "\n",
        "        for i in range(len(tri_list)):    \n",
        "            x = int(tri_list[i][0])\n",
        "            y = int(tri_list[i][1])\n",
        "            z = int(tri_list[i][2])\n",
        "            \n",
        "            t1 = [points1[x], points1[y], points1[z]]\n",
        "            t2 = [points2[x], points2[y], points2[z]]\n",
        "            t = [points[x], points[y], points[z]]\n",
        "\n",
        "            # Morph one triangle at a time.\n",
        "            morph_triangle(img1, img2, morphed_frame, t1, t2, t, alpha)\n",
        "            \n",
        "            pt1 = (int(t[0][0]), int(t[0][1]))\n",
        "            pt2 = (int(t[1][0]), int(t[1][1]))\n",
        "            pt3 = (int(t[2][0]), int(t[2][1]))\n",
        "\n",
        "            cv2.line(morphed_frame, pt1, pt2, (255, 255, 255), 1, 8, 0)\n",
        "            cv2.line(morphed_frame, pt2, pt3, (255, 255, 255), 1, 8, 0)\n",
        "            cv2.line(morphed_frame, pt3, pt1, (255, 255, 255), 1, 8, 0)\n",
        "            \n",
        "        res = Image.fromarray(cv2.cvtColor(np.uint8(morphed_frame), cv2.COLOR_BGR2RGB))\n",
        "        res.save(p.stdin,'JPEG')\n",
        "\n",
        "    p.stdin.close()\n",
        "    p.wait()"
      ],
      "metadata": {
        "id": "Qb_B06VUpJM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoFaceFound(Exception):\n",
        "   \"\"\"Raised when there is no face found\"\"\"\n",
        "   pass\n",
        "\n",
        "def calculate_margin_help(img1,img2):\n",
        "    size1 = img1.shape\n",
        "    size2 = img2.shape\n",
        "    diff0 = abs(size1[0]-size2[0])//2\n",
        "    diff1 = abs(size1[1]-size2[1])//2\n",
        "    avg0 = (size1[0]+size2[0])//2\n",
        "    avg1 = (size1[1]+size2[1])//2\n",
        "\n",
        "    return [size1,size2,diff0,diff1,avg0,avg1]\n",
        "\n",
        "def crop_image(img1,img2):\n",
        "    [size1,size2,diff0,diff1,avg0,avg1] = calculate_margin_help(img1,img2)\n",
        "\n",
        "    if(size1[0] == size2[0] and size1[1] == size2[1]):\n",
        "        return [img1,img2]\n",
        "\n",
        "    elif(size1[0] <= size2[0] and size1[1] <= size2[1]):\n",
        "        scale0 = size1[0]/size2[0]\n",
        "        scale1 = size1[1]/size2[1]\n",
        "        if(scale0 > scale1):\n",
        "            res = cv2.resize(img2,None,fx=scale0,fy=scale0,interpolation=cv2.INTER_AREA)\n",
        "        else:\n",
        "            res = cv2.resize(img2,None,fx=scale1,fy=scale1,interpolation=cv2.INTER_AREA)\n",
        "        return crop_image_help(img1,res)\n",
        "\n",
        "    elif(size1[0] >= size2[0] and size1[1] >= size2[1]):\n",
        "        scale0 = size2[0]/size1[0]\n",
        "        scale1 = size2[1]/size1[1]\n",
        "        if(scale0 > scale1):\n",
        "            res = cv2.resize(img1,None,fx=scale0,fy=scale0,interpolation=cv2.INTER_AREA)\n",
        "        else:\n",
        "            res = cv2.resize(img1,None,fx=scale1,fy=scale1,interpolation=cv2.INTER_AREA)\n",
        "        return crop_image_help(res,img2)\n",
        "\n",
        "    elif(size1[0] >= size2[0] and size1[1] <= size2[1]):\n",
        "        return [img1[diff0:avg0,:],img2[:,-diff1:avg1]]\n",
        "    \n",
        "    else:\n",
        "        return [img1[:,diff1:avg1],img2[-diff0:avg0,:]]\n",
        "\n",
        "def crop_image_help(img1,img2):\n",
        "    [size1,size2,diff0,diff1,avg0,avg1] = calculate_margin_help(img1,img2)\n",
        "    \n",
        "    if(size1[0] == size2[0] and size1[1] == size2[1]):\n",
        "        return [img1,img2]\n",
        "\n",
        "    elif(size1[0] <= size2[0] and size1[1] <= size2[1]):\n",
        "        return [img1,img2[-diff0:avg0,-diff1:avg1]]\n",
        "\n",
        "    elif(size1[0] >= size2[0] and size1[1] >= size2[1]):\n",
        "        return [img1[diff0:avg0,diff1:avg1],img2]\n",
        "\n",
        "    elif(size1[0] >= size2[0] and size1[1] <= size2[1]):\n",
        "        return [img1[diff0:avg0,:],img2[:,-diff1:avg1]]\n",
        "\n",
        "    else:\n",
        "        return [img1[:,diff1:avg1],img2[diff0:avg0,:]]\n",
        "\n",
        "def generate_face_correspondences(theImage1, theImage2):\n",
        "    # Detect the points of face.\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor('/content/shape_predictor_68_face_landmarks.dat')\n",
        "    corresp = np.zeros((68,2))\n",
        "\n",
        "    imgList = crop_image(theImage1,theImage2)\n",
        "    list1 = []\n",
        "    list2 = []\n",
        "    j = 1\n",
        "\n",
        "    for img in imgList:\n",
        "\n",
        "        size = (img.shape[0],img.shape[1])\n",
        "        if(j == 1):\n",
        "            currList = list1\n",
        "        else:\n",
        "            currList = list2\n",
        "\n",
        "        # Ask the detector to find the bounding boxes of each face. The 1 in the\n",
        "        # second argument indicates that we should upsample the image 1 time. This\n",
        "        # will make everything bigger and allow us to detect more faces.\n",
        "\n",
        "        dets = detector(img, 1)\n",
        "\n",
        "        try:\n",
        "            if len(dets) == 0:\n",
        "                raise NoFaceFound\n",
        "        except NoFaceFound:\n",
        "            print(\"Sorry, but I couldn't find a face in the image.\")\n",
        "\n",
        "        j=j+1\n",
        "\n",
        "        for k, rect in enumerate(dets):\n",
        "            \n",
        "            # Get the landmarks/parts for the face in rect.\n",
        "            shape = predictor(img, rect)\n",
        "            # corresp = face_utils.shape_to_np(shape)\n",
        "            \n",
        "            for i in range(0,68):\n",
        "                x = shape.part(i).x\n",
        "                y = shape.part(i).y\n",
        "                currList.append((x, y))\n",
        "                corresp[i][0] += x\n",
        "                corresp[i][1] += y\n",
        "                # cv2.circle(img, (x, y), 2, (0, 255, 0), 2)\n",
        "\n",
        "            # Add back the background\n",
        "            currList.append((1,1))\n",
        "            currList.append((size[1]-1,1))\n",
        "            currList.append(((size[1]-1)//2,1))\n",
        "            currList.append((1,size[0]-1))\n",
        "            currList.append((1,(size[0]-1)//2))\n",
        "            currList.append(((size[1]-1)//2,size[0]-1))\n",
        "            currList.append((size[1]-1,size[0]-1))\n",
        "            currList.append(((size[1]-1),(size[0]-1)//2))\n",
        "\n",
        "    # Add back the background\n",
        "    narray = corresp/2\n",
        "    narray = np.append(narray,[[1,1]],axis=0)\n",
        "    narray = np.append(narray,[[size[1]-1,1]],axis=0)\n",
        "    narray = np.append(narray,[[(size[1]-1)//2,1]],axis=0)\n",
        "    narray = np.append(narray,[[1,size[0]-1]],axis=0)\n",
        "    narray = np.append(narray,[[1,(size[0]-1)//2]],axis=0)\n",
        "    narray = np.append(narray,[[(size[1]-1)//2,size[0]-1]],axis=0)\n",
        "    narray = np.append(narray,[[size[1]-1,size[0]-1]],axis=0)\n",
        "    narray = np.append(narray,[[(size[1]-1),(size[0]-1)//2]],axis=0)\n",
        "    \n",
        "    return [size,imgList[0],imgList[1],list1,list2,narray]"
      ],
      "metadata": {
        "id": "uOFy91OmsK7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a point is inside a rectangle\n",
        "def rect_contains(rect, point):\n",
        "\n",
        "    if point[0] < rect[0]:\n",
        "        return False\n",
        "    elif point[1] < rect[1]:\n",
        "        return False\n",
        "    elif point[0] > rect[2]:\n",
        "        return False\n",
        "    elif point[1] > rect[3]:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Write the delaunay triangles into a file\n",
        "def draw_delaunay(f_w, f_h, subdiv, dictionary1):\n",
        "\n",
        "    list4 = []\n",
        "\n",
        "    triangleList = subdiv.getTriangleList()\n",
        "    r = (0, 0, f_w, f_h)\n",
        "\n",
        "    for t in triangleList :\n",
        "        pt1 = (int(t[0]), int(t[1]))\n",
        "        pt2 = (int(t[2]), int(t[3]))\n",
        "        pt3 = (int(t[4]), int(t[5]))\n",
        "\n",
        "        if rect_contains(r, pt1) and rect_contains(r, pt2) and rect_contains(r, pt3) :\n",
        "            list4.append((dictionary1[pt1],dictionary1[pt2],dictionary1[pt3]))\n",
        "\n",
        "    dictionary1 = {}\n",
        "    return list4\n",
        "\n",
        "def make_delaunay(f_w, f_h, theList, img1, img2):\n",
        "\n",
        "    # Make a rectangle.\n",
        "    rect = (0, 0, f_w, f_h)\n",
        "\n",
        "    # Create an instance of Subdiv2D.\n",
        "    subdiv = cv2.Subdiv2D(rect)\n",
        "\n",
        "    # Make a points list and a searchable dictionary. \n",
        "    theList = theList.tolist()\n",
        "    points = [(int(x[0]),int(x[1])) for x in theList]\n",
        "    dictionary = {x[0]:x[1] for x in list(zip(points, range(76)))}\n",
        "    \n",
        "    # Insert points into subdiv\n",
        "    for p in points :\n",
        "        subdiv.insert(p)\n",
        "\n",
        "    # Make a delaunay triangulation list.\n",
        "    list4 = draw_delaunay(f_w, f_h, subdiv, dictionary)\n",
        "   \n",
        "    # Return the list.\n",
        "    return list4"
      ],
      "metadata": {
        "id": "2bHYD07rsadt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doMorphing(img1, img2, duration, frame_rate, output):\n",
        "\n",
        "\t[size, img1, img2, points1, points2, list3] = generate_face_correspondences(img1, img2)\n",
        "\n",
        "\ttri = make_delaunay(size[1], size[0], list3, img1, img2)\n",
        "\n",
        "\tgenerate_morph_sequence(duration, frame_rate, img1, img2, points1, points2, tri, size, output)"
      ],
      "metadata": {
        "id": "NQS9BaBksgbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image1 = cv2.imread(\"/content/aligned_dir/human0_01.png\")\n",
        "image2 = cv2.imread(\"/content/aligned_dir/human1_01.png\")\n",
        "\n",
        "doMorphing(image1, image2, 5, 20, \"/content/triangle_morphing.mp4\")"
      ],
      "metadata": {
        "id": "dzdKpO2-skJF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}